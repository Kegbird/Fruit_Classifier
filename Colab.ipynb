{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"msa_project.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1j5sjMck44PP1W3NMsD6uFDIMia0J1tzf","authorship_tag":"ABX9TyOZwPQdTGKQocUYLeSOZ0lk"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5zTLHdonjkK-"},"source":["Required dependecies"]},{"cell_type":"code","metadata":{"id":"KsuRfV1NvUj-"},"source":["!pip install --upgrade pip\n","!pip install --upgrade tensorflow\n","!pip install -q kaggle\n","!pip install zipfile36"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9W14MrTzpJ9m","executionInfo":{"elapsed":10046,"status":"ok","timestamp":1629298707351,"user":{"displayName":"Pietro Prebianca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikKUYUmtPLkxaCDgySFLD8CcEdiuU7Kg5ZGz0zdQ=s64","userId":"01752184781808288382"},"user_tz":-120},"outputId":"9ea1d0f7-3362-4e69-b434-6a86432d1760"},"source":["!mkdir checkpoints\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle datasets download -d moltean/fruits"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘checkpoints’: File exists\n","Downloading fruits.zip to /content\n"," 99% 1.06G/1.08G [00:08<00:00, 112MB/s] \n","100% 1.08G/1.08G [00:08<00:00, 139MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BKRZD2zyzITa"},"source":["import zipfile\n","with zipfile.ZipFile('fruits.zip', 'r') as zipObj:\n","   zipObj.extractall()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kelrBPQH-LM5"},"source":["# **Extraction of types from \"fruits-360\" dataset**"]},{"cell_type":"code","metadata":{"id":"NNGZsWXl-KxR"},"source":["!mkdir fruits-360-type\n","!mkdir fruits-360-type/Test\n","!mkdir fruits-360-type/Training\n","\n","import shutil\n","import os\n","\n","soruce_dir_test=\"fruits-360_dataset/fruits-360/Test/\"\n","soruce_dir_training=\"fruits-360_dataset/fruits-360/Training/\"\n","test_dir=\"fruits-360-type/Test/\"\n","train_dir=\"fruits-360-type/Training/\"\n","\n","def extract_types(directory, destination):\n","  type_counter={'apple':0,'banana':0,'cherry':0,'grape':0,'peach':0,'pear':0,'pepper':0,'plum':0,'potato':0,'tomato':0}\n","  for label_type in ['apple','banana','cherry','grape','peach','pear','pepper','plum','potato','tomato']:\n","    for label_variety in os.listdir(directory):\n","      if(label_variety.lower().find(label_type)==0 and label_variety.lower().find(\"grapefruit\")!=0):\n","          image_list=os.listdir(os.path.join(directory,label_variety))\n","          for image in image_list:\n","              shutil.copy(os.path.join(directory,label_variety,image), os.path.join(destination,label_type,f\"{label_type}_{type_counter[label_type]}.jpg\"))\n","              type_counter[label_type]+=1\n","      else:\n","        continue\n","\n","for label in ['apple','banana','cherry','grape','peach','pear','pepper','plum','potato','tomato']:\n","  os.system(f\"mkdir {os.path.join(test_dir,label)}\")\n","  os.system(f\"mkdir {os.path.join(train_dir,label)}\")\n","\n","extract_types(soruce_dir_test, test_dir)\n","extract_types(soruce_dir_training, train_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0ABOgHN2PkQ"},"source":["# **Classification based on fruit and vegetable types**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":884},"id":"kxMP49lr2aqY","executionInfo":{"elapsed":91530,"status":"ok","timestamp":1601113333738,"user":{"displayName":"Pietro Prebianca","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikKUYUmtPLkxaCDgySFLD8CcEdiuU7Kg5ZGz0zdQ=s64","userId":"01752184781808288382"},"user_tz":-120},"outputId":"2746960d-9f21-4f0e-9af2-9a56c4ead873"},"source":["import numpy as np\n","import os\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow import keras\n","import tensorflow.keras.preprocessing.image\n","from tensorflow.keras import activations\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","train_data=[]\n","train_label=[]\n","test_data=[]\n","test_label=[]\n","epochs=10\n","batch_size=16\n","optimizer=\"adam\"\n","img_size = (32, 32)\n","shape = (32, 32, 3)\n","test_dir = 'fruits-360-type/Test'\n","train_dir = 'fruits-360-type/Training'\n","metrics=['accuracy']\n","\n","labels = os.listdir(train_dir)\n","num_classes = len(labels)\n","\n","def calculate_class_weights(train_label):\n","    train_size=len(train_label)\n","    weights={}\n","    for i in range(10):\n","        weights[i]=0\n","    for i in train_label:\n","        weights[i]=weights[i]+1\n","    for i in range(num_classes):\n","        weights[i]=train_size/(weights[i]*2.)\n","    return weights\n","\n","def load_data(dir):\n","    i=0\n","    data=[]\n","    data_label=[]\n","    for label in labels:\n","        images=os.listdir(os.path.join(dir,label))\n","        for image in images:\n","            image_path=os.path.join(dir,label,image)\n","            image=load_img(image_path,color_mode='rgb', target_size=img_size)\n","            array_image=img_to_array(image)\n","            array_image=array_image/255.0\n","            data.append(array_image)\n","            data_label.append(i)\n","        i=i+1\n","    tmp=list(zip(data, data_label))\n","    tmp=random(tmp)\n","    data, data_label=zip(*tmp)\n","    return data, data_label\n","\n","print(\"Loading training images...\")\n","train_data, train_label=load_data(train_dir)\n","train_data=np.array(train_data)\n","train_label=np.array(train_label)\n","print(\"Loading test images...\")\n","test_data, test_label=load_data(test_dir)\n","test_data=np.array(test_data)\n","test_label=np.array(test_label)\n","checkpoint_dir = \"checkpoints/32_64_128_Filters_512_Dense.h5\"\n","callbacks=[ModelCheckpoint(checkpoint_dir, monitor='loss', verbose=0, save_best_only=True,save_weights_only=False, mode='auto', save_freq='epoch')]\n","network=tf.keras.models.Sequential()\n","network.add(Input(shape=shape))\n","network.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", strides=(1, 1)))\n","network.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","network.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", strides=(1, 1)))\n","network.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","network.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", strides=(1, 1)))\n","network.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","network.add(Flatten())\n","network.add(Dense(512, activation=\"softmax\"))\n","network.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=metrics)\n","\n","print(network.summary())\n","weights=calculate_class_weights(train_label)\n","history = network.fit(x=train_data,y=train_label,\n","                      batch_size=batch_size,\n","                      epochs=epochs,\n","                      verbose=1,\n","                      callbacks=callbacks)\n","train_loss, train_accuracy = network.evaluate(x=train_data,y=train_label,steps=(len(train_data)//batch_size)+1, verbose=1)\n","test_loss, test_accuracy = network.evaluate(x=test_data, y=test_label,steps=(len(test_data)//batch_size)+1, verbose=1)\n","print(f\"Train accuracy = {train_accuracy}\")\n","print(f\"Test accuracy = {test_accuracy}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading training images...\n","Loading test images...\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_3 (Conv2D)            (None, 32, 32, 32)        896       \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 2048)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               1049088   \n","=================================================================\n","Total params: 1,142,336\n","Trainable params: 1,142,336\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 0.2683 - accuracy: 0.9117\n","Epoch 2/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 0.0261 - accuracy: 0.9925\n","Epoch 3/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 0.0148 - accuracy: 0.9954\n","Epoch 4/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 0.0039 - accuracy: 0.9988\n","Epoch 5/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 3.8075e-05 - accuracy: 1.0000\n","Epoch 6/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 1.0856e-05 - accuracy: 1.0000\n","Epoch 7/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 3.8464e-06 - accuracy: 1.0000\n","Epoch 8/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 1.4073e-06 - accuracy: 1.0000\n","Epoch 9/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 4.8789e-07 - accuracy: 1.0000\n","Epoch 10/10\n","2038/2038 [==============================] - 7s 3ms/step - loss: 1.6127e-07 - accuracy: 1.0000\n","2038/2038 [==============================] - 4s 2ms/step - loss: 1.0227e-07 - accuracy: 1.0000\n","682/682 [==============================] - 1s 2ms/step - loss: 0.0281 - accuracy: 0.9918\n","Train accuracy = 1.0\n","Test accuracy = 0.9918393492698669\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p5uDAotI_YZz"},"source":["# **Classification based on fruit and vegetable varieties**"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"BfV5tglGDzDY"},"source":["import numpy as np\n","import os\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow import keras\n","from tensorflow.keras import activations\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","from tensorflow.keras.callbacks import  ModelCheckpoint\n","import coremltools as ct\n","\n","train_data=[]\n","train_label=[]\n","test_data=[]\n","test_label=[]\n","epochs=20\n","batch_size=16\n","optimizer=\"adam\"\n","img_size = (32, 32)\n","shape = (32, 32, 3)\n","test_dir = 'fruits-360_dataset/fruits-360/Test'\n","train_dir = 'fruits-360_dataset/fruits-360/Training'\n","metrics=['accuracy']\n","\n","labels = os.listdir(train_dir)\n","print(labels)\n","num_classes = len(labels)\n","\n","def calculate_class_weights(train_label, num_classes):\n","    train_size=len(train_label)\n","    weights={}\n","    for i in range(num_classes):\n","        weights[i]=0\n","    for i in train_label:\n","        weights[i]=weights[i]+1\n","    for i in range(num_classes):\n","        weights[i]=train_size/(weights[i]*2.)\n","    return weights\n","\n","def load_data(dir, labels):\n","    i=0\n","    data=[]\n","    data_label=[]\n","    for label in labels:\n","        images=os.listdir(os.path.join(dir,label))\n","        for image in images:\n","            image_path=os.path.join(dir,label,image)\n","            image=load_img(image_path,color_mode='rgb', target_size=img_size)\n","            array_image=img_to_array(image)\n","            array_image=array_image/255.0\n","            data.append(array_image)\n","            data_label.append(i)\n","        i=i+1\n","    tmp=list(zip(data, data_label))\n","    data, data_label=zip(*tmp)\n","    return data, data_label\n","\n","print(\"Loading training images...\")\n","train_data, train_label=load_data(train_dir, labels)\n","train_data=np.array(train_data)\n","train_label=np.array(train_label)\n","print(\"Loading test images...\")\n","test_data, test_label=load_data(test_dir, labels)\n","test_data=np.array(test_data)\n","test_label=np.array(test_label)\n","checkpoint_dir = \"checkpoints/32_64_Filters_512_Dense_50%_Dropout.ckpt\"\n","callbacks=[ModelCheckpoint(checkpoint_dir, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True)]\n","\n","network=tf.keras.models.Sequential()\n","network.add(Input(shape=shape))\n","network.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", strides=(1, 1)))\n","network.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","network.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", strides=(1, 1)))\n","network.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","network.add(Flatten())\n","network.add(Dense(512, activation=\"relu\"))\n","network.add(Dropout(0.5))\n","network.add(Dense(num_classes, activation=\"softmax\"))\n","network.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=metrics)\n","print(network.summary())\n","weights=calculate_class_weights(train_label, num_classes)\n","history = network.fit(x=train_data,y=train_label,\n","                        batch_size=batch_size,\n","                        epochs=epochs,\n","                        class_weight=weights,\n","                        verbose=1,\n","                        callbacks=callbacks)\n","network.load_weights(checkpoint_dir)\n","train_loss, train_accuracy = network.evaluate(x=train_data,y=train_label, verbose=1)\n","test_loss, test_accuracy = network.evaluate(x=test_data, y=test_label, verbose=1)\n","predicted=network.predict(test_data, batch_size=16, verbose=1)\n","predicted = predicted.argmax(axis=-1)\n","print(f\"Train accuracy = {train_accuracy}\")\n","print(f\"Test accuracy = {test_accuracy}\")"],"execution_count":null,"outputs":[]}]}